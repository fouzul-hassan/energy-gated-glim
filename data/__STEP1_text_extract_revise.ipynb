{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract stimulus texts and original labels\n",
    "Set up `data_dir` and run the following blocks one-by-one to:\n",
    "1. extract texts and labels from `.csv` files;\n",
    "2. revise some known typos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_dir = './raw_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZuCo1-task1 (Normal Reading)\n",
    "- Copy all `...ZuCo1/task_materials/xxx.csv` files into a new created dir (e.g., `.../ZuCo1/revised_csv/`).\n",
    "- There are several format errors in some original `.csv` files (e.g., absence of column headers), you may take a few minutes to *manually correct* them according to the ERROR messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\MSc Files\\\\MSc Project\\\\E2T-w-VJEPA\\\\glim-way\\\\GLIM\\\\data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_1 = './raw_data/ZuCo1/revised_csv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 5) Index(['raw text', 'dataset', 'task', 'control', 'raw label'], dtype='object')\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df11_raw = pd.read_csv('./raw_data/ZuCo1/revised_csv/sentiment_labels_task1.csv', \n",
    "                       sep=';', header=0,  skiprows=[1], encoding='utf-8',\n",
    "                       dtype={'sentence': str, 'control': str, 'sentiment_label':str})\n",
    "# print(df1_raw)\n",
    "# n_row, n_column = df11_raw.shape\n",
    "df11 = df11_raw.rename(columns={'sentence': 'raw text', \n",
    "                            'sentiment_label': 'raw label'})\n",
    "df11 = df11.reindex(columns=['raw text', 'dataset', 'task', 'control', 'raw label',])\n",
    "                      \n",
    "df11['dataset'] =  ['ZuCo1'] * df11.shape[0]  # each item is init as a tuple with len==1 for easy extension\n",
    "df11['task'] =  ['task1'] * df11.shape[0]\n",
    "df11['control'] = df11['control'].apply(lambda x: x == 'CONTROL')\n",
    "print(df11.shape, df11.columns)\n",
    "print(df11['raw text'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZuCo1-task2 (Normal Reading)  \n",
    "Note: there are multiple relation labels in some of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 5) Index(['raw text', 'dataset', 'task', 'control', 'raw label'], dtype='object')\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "def reformat_relation_types(text):\n",
    "    '''\n",
    "    `VISITED` --> tuple(`VISITED`,)\n",
    "    `AWARD;JOB_TITLE;NATIONALITY` --> tuple(`AWARD`,`JOB_TITLE`,`NATIONALITY`)\n",
    "    `NO-RELATION` --> np.nan\n",
    "    '''\n",
    "    assert isinstance(text, str)\n",
    "    if text == 'NO-RELATION':\n",
    "        text = np.nan\n",
    "    else:\n",
    "        text = tuple(text.split(';'))\n",
    "    return text\n",
    "\n",
    "df12_raw = pd.read_csv(data_dir_1 + 'relations_labels_task2.csv', \n",
    "                       sep=',', header=0, encoding='utf-8',\n",
    "                       dtype={'sentence': str,'control': str,'relation_types':str})\n",
    "# n_row, n_column = df12_raw.shape\n",
    "df12 = df12_raw.rename(columns={'sentence': 'raw text', \n",
    "                                'relation_types': 'raw label'})\n",
    "df12 = df12.reindex(columns=['raw text', 'dataset', 'task', 'control', 'raw label',])\n",
    "df12['dataset'] =  ['ZuCo1'] * df12.shape[0]\n",
    "df12['task'] =  ['task2'] * df12.shape[0]\n",
    "df12['control'] = df12['control'].apply(lambda x: x == 'CONTROL')\n",
    "df12['raw label'] = df12['raw label'].apply(reformat_relation_types)\n",
    "print(df12.shape, df12.columns)\n",
    "print(df12['raw text'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZuCo1-task3 (Task-specific Reading)  \n",
    "Note: there are repeated sentences yet with different relation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(407, 5) Index(['raw text', 'dataset', 'task', 'control', 'raw label'], dtype='object')\n",
      "386\n"
     ]
    }
   ],
   "source": [
    "# def assign_control_with_label(label):\n",
    "#     assert label in ['AWARD', 'EDUCATION', 'EMPLOYER', \n",
    "#                    'FOUNDER', 'JOB_TITLE', 'NATIONALITY', \n",
    "#                    'POLITICAL_AFFILIATION', 'VISITED', 'WIFE',\n",
    "#                    'CONTROL']\n",
    "#     return True if label == 'CONTROL' else False\n",
    "\n",
    "# df13_raw = pd.read_csv(data_dir_1 + '/relations_labels_task3.csv', \n",
    "#                        sep=';', header=0, encoding='utf-8', \n",
    "#                        dtype={'sentence': str, 'relation-type':str})\n",
    "# df13 = df13_raw.rename(columns={'sentence': 'raw text', \n",
    "#                             'relation-type': 'raw label'})\n",
    "# df13 = df13.reindex(columns=['raw text', 'dataset', 'task', 'control', 'raw label',])\n",
    "# df13['dataset'] =  ['ZuCo1'] * df13.shape[0]\n",
    "# df13['task'] =  ['task3'] * df13.shape[0]\n",
    "# df13['control'] = df13['raw label'].apply(assign_control_with_label)\n",
    "# # df13['label'] = df13['label'].apply(lambda x: x if x!='CONTROL' else np.nan)\n",
    "# for i in range(df13.shape[0]):\n",
    "#     label = df13.loc[i, 'raw label']\n",
    "#     if label == 'CONTROL':\n",
    "#         left = df13.loc[i-1, 'raw label']\n",
    "#         right = df13.loc[i+1, 'raw label']\n",
    "#         assert left == right\n",
    "#         df13.loc[i, 'raw label'] = left\n",
    "\n",
    "# print(df13.shape, df13.columns)\n",
    "# print(df13['raw text'].nunique())\n",
    "def assign_control_with_label(label):\n",
    "    assert label in ['AWARD', 'EDUCATION', 'EMPLOYER', \n",
    "                   'FOUNDER', 'JOB_TITLE', 'NATIONALITY', \n",
    "                   'POLITICAL_AFFILIATION', 'VISITED', 'WIFE',\n",
    "                   'CONTROL']\n",
    "    return True if label == 'CONTROL' else False\n",
    "\n",
    "df13_raw = pd.read_csv(data_dir_1 + '/relations_labels_task3.csv', \n",
    "                       sep=';', header=0, encoding='utf-8', \n",
    "                       dtype={'sentence': str, 'relation-type':str})\n",
    "df13 = df13_raw.rename(columns={'sentence': 'raw text', \n",
    "                            'relation-type': 'raw label'})\n",
    "df13 = df13.reindex(columns=['raw text', 'dataset', 'task', 'control', 'raw label',])\n",
    "df13['dataset'] =  ['ZuCo1'] * df13.shape[0]\n",
    "df13['task'] =  ['task3'] * df13.shape[0]\n",
    "df13['control'] = df13['raw label'].apply(assign_control_with_label)\n",
    "\n",
    "# ✅ FIX: Reset the index to ensure 0, 1, 2, ... indexing\n",
    "df13 = df13.reset_index(drop=True)\n",
    "\n",
    "for i in range(df13.shape[0]):\n",
    "    label = df13.loc[i, 'raw label']\n",
    "    if label == 'CONTROL':\n",
    "        left = df13.loc[i-1, 'raw label']\n",
    "        right = df13.loc[i+1, 'raw label']\n",
    "        assert left == right\n",
    "        df13.loc[i, 'raw label'] = left\n",
    "\n",
    "print(df13.shape, df13.columns)\n",
    "print(df13['raw text'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZuCo2-task2 (Normal Reading)  \n",
    "Note: there repeated sentences with unkown labels, we will drop them at the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 Index(['raw text', 'dataset', 'task', 'control', 'raw label'], dtype='object')\n",
      "365\n"
     ]
    }
   ],
   "source": [
    "def extract_merge(file_dir, n=1):\n",
    "    sentence_path = file_dir + f'/nr_{n}.csv'\n",
    "    control_path = file_dir + f'/nr_{n}_control_questions.csv'\n",
    "    df_raw = pd.read_csv(sentence_path, sep=';', encoding='utf-8', header=None,\n",
    "                         names = ['paragraph_id', 'sentence_id','sentence','control'],\n",
    "                         dtype={'paragraph_id':str, 'sentence_id': str, 'sentence': str, 'control': str})\n",
    "    df_control = pd.read_csv(control_path, sep=';', encoding='utf-8', header=0,\n",
    "                             dtype={'paragraph_id':str, 'sentence_id': str,'control_question': str, 'correct_answer':str})\n",
    "    assert df_raw[df_raw['control']=='CONTROL'].shape[0] == df_control.shape[0]\n",
    "    df = pd.merge(df_raw, df_control, how='left', on=['paragraph_id', 'sentence_id'])\n",
    "    return df\n",
    "\n",
    "def merge_QA(q,a):\n",
    "    if pd.isna(q):\n",
    "        label = np.nan\n",
    "    else:\n",
    "        if q.endswith('...'):\n",
    "            label = q.replace('...', ' '+a)\n",
    "        elif q.endswith('?'):\n",
    "            label = q + ' ' + a\n",
    "        else:\n",
    "            raise ValueError\n",
    "    return label\n",
    "\n",
    "file_dir = './raw_data/ZuCo2/task_materials'\n",
    "df22_list = []\n",
    "for i in range(1,8):\n",
    "    df = extract_merge(file_dir, i)\n",
    "    df22_list.append(df)\n",
    "df22 = pd.concat(df22_list, ignore_index=True,)\n",
    "\n",
    "labels=[]\n",
    "for i in range(df22.shape[0]):\n",
    "    label = merge_QA(df22['control_question'][i], df22['correct_answer'][i])\n",
    "    labels.append(label)\n",
    "df22['raw label'] = labels\n",
    "df22['control'] = df22['control'].apply(lambda x: x == 'CONTROL')\n",
    "\n",
    "df22 = df22.rename(columns={'sentence': 'raw text'})\n",
    "df22 = df22.reindex(columns=['raw text', 'dataset', 'task', 'control', 'raw label',])\n",
    "df22['dataset'] =  ['ZuCo2'] * df22.shape[0]\n",
    "df22['task'] =  ['task2'] * df22.shape[0]\n",
    "print(df22.shape[0], df22.columns)\n",
    "print(df22['raw text'].nunique())\n",
    "# print(df22['raw text'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZuCo2-task3 (Task-specific Reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411 Index(['raw text', 'dataset', 'task', 'control', 'raw label'], dtype='object')\n",
      "392\n"
     ]
    }
   ],
   "source": [
    "def extract_task3(file_dir, n=1):\n",
    "    file_path = file_dir + f'/tsr_{n}.csv'\n",
    "    df_raw = pd.read_csv(file_path, sep=';', encoding='utf-8', header=None,\n",
    "                         names = ['paragraph_id', 'sentence_id', 'sentence', 'label'],\n",
    "                         dtype={'paragraph_id':str, 'sentence_id': str, 'sentence': str, 'label': str})\n",
    "    df = df_raw.rename(columns={'sentence': 'raw text', \n",
    "                                'label': 'raw label'})\n",
    "    df = df.reindex(columns=['raw text', 'dataset', 'task', 'control', 'raw label',])\n",
    "    df['control'] = df['raw label'].apply(assign_control_with_label)\n",
    "    unique_labels = df['raw label'].unique().tolist()\n",
    "    unique_labels.remove('CONTROL')\n",
    "    assert len(unique_labels) == 1\n",
    "    df['raw label'] =  unique_labels * df.shape[0]\n",
    "    df['dataset'] =  ['ZuCo2'] * df.shape[0]\n",
    "    df['task'] =  ['task3'] * df.shape[0]\n",
    "    return df\n",
    "\n",
    "def assign_control_with_label(label):\n",
    "    assert label in ['AWARD', 'EDUCATION', 'EMPLOYER', \n",
    "                   'FOUNDER', 'JOB_TITLE', 'NATIONALITY', \n",
    "                   'POLITICAL_AFFILIATION', 'VISITED', 'WIFE',\n",
    "                   'CONTROL']\n",
    "    return True if label == 'CONTROL' else False\n",
    "\n",
    "file_dir = data_dir + '/ZuCo2/task_materials'\n",
    "df23_list = []\n",
    "for i in range(1,8):\n",
    "    df = extract_task3(file_dir,i)\n",
    "    df23_list.append(df)\n",
    "df23 = pd.concat(df23_list, ignore_index=True,)\n",
    "print(df23.shape[0], df23.columns)\n",
    "print(df23['raw text'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat sub-tables and revise typos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1888, 5) Index(['raw text', 'dataset', 'task', 'control', 'raw label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df11, df12, df13, df22, df23], ignore_index=True,)\n",
    "print(df.shape, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1477, 5) Index(['raw text', 'dataset', 'task', 'control', 'raw label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df11, df12, df13, df22], ignore_index=True,)\n",
    "print(df.shape, df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revise each `raw text` according the typos we identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'float' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m             text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(src, tgt)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m---> 36\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(revise_typo)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique(), df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[15], line 32\u001b[0m, in \u001b[0;36mrevise_typo\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     30\u001b[0m book \u001b[38;5;241m=\u001b[39m typobook\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m src, tgt \u001b[38;5;129;01min\u001b[39;00m book\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m src \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[0;32m     33\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(src, tgt)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'float' is not iterable"
     ]
    }
   ],
   "source": [
    "# typobook = {\"emp11111ty\":   \"empty\",\n",
    "#             \"film.1\":       \"film.\",\n",
    "#             \"–\":            \"-\",\n",
    "#             \"’s\":           \"'s\",\n",
    "#             \"�s\":           \"'s\",\n",
    "#             \"`s\":           \"'s\",\n",
    "#             \"Maria\":        \"Marić\",\n",
    "#             \"1Universidad\": \"Universidad\",\n",
    "#             \"1902—19\":      \"1902 - 19\",\n",
    "#             \"Wuerttemberg\": \"Württemberg\",\n",
    "#             \"long -time\":   \"long-time\",\n",
    "#             \"Jose\":         \"José\",\n",
    "#             \"Bucher\":       \"Bôcher\",\n",
    "#             \"1839 ? May\":   \"1839 - May\",\n",
    "#             \"G�n�ration\":  \"Generation\",\n",
    "#             \"Bragança\":     \"Bragana\",\n",
    "#             \"1837?October\": \"1837 - October\",\n",
    "#             \"nVera-Ellen\":  \"Vera-Ellen\",\n",
    "#             \"write Ethics\": \"wrote Ethics\",\n",
    "#             \"Adams-Onis\":   \"Adams-Onís\",\n",
    "#             \"(40 km?)\":     \"(40 km²)\",\n",
    "#             \"(40 km˝)\":     \"(40 km²)\",\n",
    "#             \" (IPA: /?g?nz?b?g/) \": \" \",\n",
    "#             '\"\"Canes\"\"':    '\"Canes\"',\n",
    "\n",
    "#             }\n",
    "\n",
    "# def revise_typo(text):\n",
    "#     # the typo book \n",
    "#     book = typobook\n",
    "#     for src, tgt in book.items():\n",
    "#         if src in text:\n",
    "#             text = text.replace(src, tgt)\n",
    "#     return text\n",
    "\n",
    "# df['input text'] = df['raw text'].apply(revise_typo)\n",
    "# print(df.columns)\n",
    "# print(df['raw text'].nunique(), df['input text'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in 'raw text': 400\n",
      "Index(['raw text', 'dataset', 'task', 'control', 'raw label', 'input text'], dtype='object')\n",
      "943 925\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "typobook = {\"emp11111ty\":   \"empty\",\n",
    "            \"film.1\":       \"film.\",\n",
    "            \"–\":            \"-\",\n",
    "            \"'s\":           \"'s\",\n",
    "            \"�s\":           \"'s\",\n",
    "            \"`s\":           \"'s\",\n",
    "            \"Maria\":        \"Marić\",\n",
    "            \"1Universidad\": \"Universidad\",\n",
    "            \"1902—19\":      \"1902 - 19\",\n",
    "            \"Wuerttemberg\": \"Württemberg\",\n",
    "            \"long -time\":   \"long-time\",\n",
    "            \"Jose\":         \"José\",\n",
    "            \"Bucher\":       \"Bôcher\",\n",
    "            \"1839 ? May\":   \"1839 - May\",\n",
    "            \"G�n�ration\":  \"Generation\",\n",
    "            \"Bragança\":     \"Bragana\",\n",
    "            \"1837?October\": \"1837 - October\",\n",
    "            \"nVera-Ellen\":  \"Vera-Ellen\",\n",
    "            \"write Ethics\": \"wrote Ethics\",\n",
    "            \"Adams-Onis\":   \"Adams-Onís\",\n",
    "            \"(40 km?)\":     \"(40 km²)\",\n",
    "            \"(40 km˝)\":     \"(40 km²)\",\n",
    "            \" (IPA: /?g?nz?b?g/) \": \" \",\n",
    "            '\"\"Canes\"\"':    '\"Canes\"',\n",
    "            }\n",
    "\n",
    "def revise_typo(text):\n",
    "    # Handle NaN/None values\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    book = typobook\n",
    "    for src, tgt in book.items():\n",
    "        if src in text:\n",
    "            text = text.replace(src, tgt)\n",
    "    return text\n",
    "\n",
    "# Check for NaN values first\n",
    "print(f\"NaN values in 'raw text': {df['raw text'].isna().sum()}\")\n",
    "\n",
    "df['input text'] = df['raw text'].apply(revise_typo)\n",
    "print(df.columns)\n",
    "print(df['raw text'].nunique(), df['input text'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(df, './tmp/zuco_label_input_text2.df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\MSc Files\\\\MSc Project\\\\E2T-w-VJEPA\\\\gated-glim\\\\GLIM\\\\data'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File exists!\n",
      "Shape: (16316, 22)\n",
      "Columns: ['eeg', 'mask', 'subject', 'label id', 'raw text', 'dataset', 'task', 'control', 'raw label', 'input text', 'text uid', 'sentiment label', 'relation label', 'lexical simplification (v0)', 'lexical simplification (v1)', 'semantic clarity (v0)', 'semantic clarity (v1)', 'syntax simplification (v0)', 'syntax simplification (v1)', 'naive rewritten', 'naive simplified', 'phase']\n",
      "✅ All required columns present!\n",
      "\n",
      "Train samples: 13023\n",
      "Val samples: 1631\n",
      "Test samples: 1662\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check if the final training file exists\n",
    "data_path = './tmp/zuco_eeg_label_8variants.df'\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_pickle(data_path)\n",
    "    print(\"✅ File exists!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required = ['eeg', 'mask', 'phase', 'input text']\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"❌ Missing required columns: {missing}\")\n",
    "    else:\n",
    "        print(\"✅ All required columns present!\")\n",
    "        print(f\"\\nTrain samples: {(df['phase']=='train').sum()}\")\n",
    "        print(f\"Val samples: {(df['phase']=='val').sum()}\")\n",
    "        print(f\"Test samples: {(df['phase']=='test').sum()}\")\n",
    "else:\n",
    "    print(f\"❌ File not found: {data_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
